{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "So far we have introduced a variety of techniques for manipulating data that are already stored in tensors.\n",
    "To apply deep learning to solving real-world problems,\n",
    "we often begin with preprocessing raw data, rather than those nicely prepared data in the tensor format.\n",
    "Among popular data analytic tools in Python, the `pandas` package is commonly used.\n",
    "Like many other extension packages in the vast ecosystem of Python,\n",
    "`pandas` can work together with tensors.\n",
    "So, we will briefly walk through steps for preprocessing raw data with `pandas`\n",
    "and converting them into the tensor format.\n",
    "We will cover more data preprocessing techniques in later chapters.\n",
    "\n",
    "## Reading the Dataset\n",
    "\n",
    "As an example,\n",
    "we begin by (**creating an artificial dataset that is stored in a\n",
    "csv (comma-separated values) file**)\n",
    "`../data/house_tiny.csv`. Data stored in other\n",
    "formats may be processed in similar ways.\n",
    "\n",
    "Below we write the dataset row by row into a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
    "    f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "To [**load the raw dataset from the created csv file**],\n",
    "we import the `pandas` package and invoke the `read_csv` function.\n",
    "This dataset has four rows and three columns, where each row describes the number of rooms (\"NumRooms\"), the alley type (\"Alley\"), and the price (\"Price\") of a house.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "# If pandas is not installed, just uncomment the following line:\n",
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Note that \"NaN\" entries are missing values.\n",
    "To handle missing data, typical methods include *imputation* and *deletion*,\n",
    "where imputation replaces missing values with substituted ones,\n",
    "while deletion ignores missing values. Here we will consider imputation.\n",
    "\n",
    "By integer-location based indexing (`iloc`), we split `data` into `inputs` and `outputs`,\n",
    "where the former takes the first two columns while the latter only keeps the last column.\n",
    "For numerical values in `inputs` that are missing,\n",
    "we [**replace the \"NaN\" entries with the mean value of the same column.**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = inputs.fillna(inputs[[\"NumRooms\"]].mean()) # before pandas 1.3.0 (now deprecated), you could just call: inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6,
    "tags": []
   },
   "source": [
    "[**For categorical or discrete values in `inputs`, we consider \"NaN\" as a category.**]\n",
    "Since the \"Alley\" column only takes two types of categorical values \"Pave\" and \"NaN\",\n",
    "`pandas` can automatically convert this column to two columns \"Alley_Pave\" and \"Alley_nan\".\n",
    "A row whose alley type is \"Pave\" will set values of \"Alley_Pave\" and \"Alley_nan\" to 1 and 0.\n",
    "A row with a missing alley type will set their values to 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0           1          0\n",
      "1       2.0           0          1\n",
      "2       4.0           0          1\n",
      "3       3.0           0          1\n"
     ]
    }
   ],
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We join the `inputs` and `outputs` and save the processed values into a new file `../data/house_tiny_processed.csv` for later purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan   Price\n",
      "0       3.0           1          0  127500\n",
      "1       2.0           0          1  106000\n",
      "2       4.0           0          1  178100\n",
      "3       3.0           0          1  140000\n"
     ]
    }
   ],
   "source": [
    "data_processed = inputs.join(outputs)\n",
    "print(data_processed)\n",
    "\n",
    "data_file = os.path.join('..', 'data', 'house_tiny_processed.csv')\n",
    "data_processed.to_csv(path_or_buf=data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## Conversion to the Tensor Format\n",
    "\n",
    "Now that [**all the entries in `inputs` and `outputs` are numerical, they can be converted to the tensor format.**]\n",
    "Once data are in this format, they can be further manipulated with those tensor functionalities that we have introduced in :numref:`sec_ndarray`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 1., 0.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 0., 1.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500, 106000, 178100, 140000]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Like many other extension packages in the vast ecosystem of Python, `pandas` can work together with tensors.\n",
    "* Imputation and deletion can be used to handle missing data.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Create a raw dataset with more rows and columns.\n",
    "\n",
    "1. Delete the column with the most missing values.\n",
    "2. Convert the preprocessed dataset to the tensor format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Datasets & DataLoaders\n",
    "PyTorch has built-in fnctionalities for creating and loading datasets: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "Read the documentation, and follow the given examples for the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34179638a14044c88a05b3007f21a0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c547b5f32258490ba17275ea33c97ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61916f099174e949ae1e8bd9e26d64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0b65a9151b4be78e76e94e5176cff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# required for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# only required, if you want to do transformations\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# load the data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=os.path.join('..', 'data'),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "    \n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Create your own custom dataset by using PyTorch's built-in functionality. Read the data from `../data/house_tiny.csv` where (\"NumRooms\") and (\"Alley\") are one tensor, and (\"Price\") is another one. Return them in the appropriated method as two values in one call (return x, y).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 1., 0.]], dtype=torch.float64) tensor([127500])\n",
      "tensor([[2., 0., 1.]], dtype=torch.float64) tensor([106000])\n",
      "tensor([[4., 0., 1.]], dtype=torch.float64) tensor([178100])\n",
      "tensor([[3., 0., 1.]], dtype=torch.float64) tensor([140000])\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# only required, if you want to do transformations\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CustomCSVDataset(Dataset):\n",
    "    def __init__(self, csv, transform=None, target_transform=None):\n",
    "        self.pd = pd.read_csv(csv)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pd)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # for pandas it's more simple to transform the whole pandas series to a Tensor by creating a new one\n",
    "        # (as you did before in the section \"Conversion to the Tensor Format\".\n",
    "        # The toTensor() function of torchvision.transforms cannot handle this datatype by default. \n",
    "        # One would need to write a custom collate function.\n",
    "        x = torch.tensor(self.pd.iloc[idx, 0:3])\n",
    "        \n",
    "        y = self.pd.iloc[idx, 3]   \n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "\n",
    "data_file = os.path.join('..', 'data', 'house_tiny_processed.csv')\n",
    "\n",
    "custom_set = CustomCSVDataset(data_file)\n",
    "\n",
    "custom_set_loader = DataLoader(custom_set, batch_size=1, shuffle=False)\n",
    "\n",
    "for x,y in custom_set_loader:\n",
    "    print(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
