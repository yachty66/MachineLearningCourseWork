{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Framework - Exercise: Fully Connected Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction) \n",
    "* [Requirements](#Requirements) \n",
    "  * [Modules](#Python-Modules) \n",
    "  * [Data](#Data)\n",
    "* [Towards a Neural Network Framework](#Towards-a-Neural-Network-Framework)\n",
    "  * [Exercise: Define Layers](#Exercise:-Define-Layers)\n",
    "  * [Exercise: Define Activation Function](#Exercise:-Define-Activation-Function)   \n",
    "  * [The NeuralNetwork Class](#NerualNetwork-Class)\n",
    "  * [Exercise: Define a Cost Function](#Exercise:-Define-a-Cost-Function)\n",
    "  * [Optimization with SGD](#Optimization-with-SGD)\n",
    "  * [Put it All Together and Train a MNIST Network](#Put-it-All-Together) \n",
    "  * [Exercise: Experiment with the Framework](#Exercise:-Experiment-with-the-Framework)\n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For a better understanding of neural networks, you will start to implement a framework on your own. The given notebook explains some core functions and concepts of the framework, so all of you have the same starting point. Our previous exercises were self-contained and not very modular. You are going to change that. Let us begin with a fully connected network on the now well-known MNIST dataset. The  Pipeline will be: \n",
    "\n",
    "* Define a model architecture \n",
    "* Construct a neural network based on the architecture\n",
    "* Define an evaluation criteria \n",
    "* Optimize the model (training)\n",
    "\n",
    "Read the whole notebook carefully to understand how the pipeline works even if there is no specific implementation work required from you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge\n",
    "\n",
    "**TODO**\n",
    "\n",
    "### Python-Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "import numpy as np\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We load the MNIST dataset. Have a look at the data structure that is necessary to use feed data into the framework. A batch is a 4d tensor with: (image_i, channel, width, height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto download is active, attempting download\n",
      "data directory does not exist, starting download...\n",
      "train-images-idx3-ubyte.gz successfully downloaded\n",
      "train-labels-idx1-ubyte.gz successfully downloaded\n",
      "t10k-images-idx3-ubyte.gz successfully downloaded\n",
      "t10k-labels-idx1-ubyte.gz successfully downloaded\n",
      "... mnist data completely downloaded, enjoy.\n",
      "(60000, 28, 28) (60000,)\n",
      "(60000, 1, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# create mnist loader from deep_teaching_commons\n",
    "mnist_loader = Mnist(data_dir='data/MNIST')\n",
    "\n",
    "# load all data, labels are not one-hot-encoded, images are flatten and pixel squashed between [0,1]\n",
    "train_images, train_labels, test_images, test_labels = mnist_loader.get_all_data(flatten=False, one_hot_enc=False, normalized=True)\n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# reshape to match general framework architecture \n",
    "train_images, test_images = train_images.reshape(60000, 1, 28, 28), test_images.reshape(10000, 1, 28, 28)            \n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "train_images, train_labels = train_images[shuffle_index], train_labels[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<deep_teaching_commons.data.fundamentals.mnist.Mnist object at 0x7f93588ec850>\n"
     ]
    }
   ],
   "source": [
    "print(mnist_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards a Neural Network Framework\n",
    "To create custom models you have to be able to define layers and activation functions in a modular way. Layers and activation functions are therefore modelled as objects. Each object that you want to use has to implement a `forward` and a `backward` method that is used later by the `NeuralNetwork` class. Additionally the `self.params` attribute is mandatory to meet the specification of the `NeuralNetwork` class. It is used to store all learnable parameters that you need for the optimization algorithm. Implemented that way you can use the objects as building blocks and stack them up to create a custom model. Be aware of using an activation function after each layer except the last one, cause the softmax function is applied by default during loss calculation of the network output. \n",
    "\n",
    "After completing this notebook you can move the implemented functions to the script files for further development. The framework consists of the following files:\n",
    "* `layer.py`\n",
    "* `activation_func.py`\n",
    "* `network.py`\n",
    "* `cost_func`\n",
    "* `optimizer.py`\n",
    "* `utils.py`\n",
    "\n",
    "After processing the notebook, it certainly becomes clear which functionality belongs into which file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Define Layers  \n",
    "The first layers added to the framework are a flatten and a fully-connected layer, which we need to build an architecture for the corresponding fully connected network â€” sidenote sometimes, depending on the framework, the term dense layer is used instead of fully-connected. \n",
    "\n",
    "All kind of neural network layers and regularization techniques that can be inserted as layers into a architecture will be implemented in the file `layer.py` later.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the methods `FullyConnected.forward` and `FullyConnected.backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used https://mlxai.github.io/2017/01/10/a-modular-approach-to-implementing-fully-connected-neural-networks.html for implementation \n",
    "class Flatten(object):\n",
    "    ''' Flatten layer used to reshape inputs into vector representation\n",
    "    \n",
    "    Layer should be used in the forward pass before a dense layer to \n",
    "    transform a given tensor into a vector. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Reshapes a n-dim representation into a vector \n",
    "            by preserving the number of input rows.\n",
    "        \n",
    "        Examples:\n",
    "            [10000,[1,28,28]] -> [10000,784]\n",
    "        '''\n",
    "        self.X_shape = X.shape\n",
    "        self.out_shape = (self.X_shape[0], -1)    \n",
    "        out = X.reshape(-1).reshape(self.out_shape)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Restore dimensions before flattening operation\n",
    "        '''\n",
    "        out = dout.reshape(self.X_shape)\n",
    "        return out, []\n",
    "\n",
    "class FullyConnected(object):\n",
    "    ''' Fully connected layer implemtenting linear function hypothesis \n",
    "        in the forward pass and its derivation in the backward pass.\n",
    "    '''\n",
    "    def __init__(self, in_size, out_size):\n",
    "        ''' initialize all learning parameters in the layer\n",
    "        \n",
    "        Weights will be initialized with modified Xavier initialization.\n",
    "        Biases will be initialized with zero. \n",
    "        '''\n",
    "        self.W = np.random.randn(in_size, out_size) * np.sqrt(2. / in_size)\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Linear combiationn of images, weights and bias terms\n",
    "            \n",
    "        Args:\n",
    "            X: Matrix of images (flatten represenation)\n",
    "    \n",
    "        Returns:\n",
    "            out: Sum of X*W+b  \n",
    "        '''\n",
    "        self.X = X\n",
    "        ############################################\n",
    "        #                   TODO                   #    \n",
    "        ############################################\n",
    "        '''\n",
    "        X = matrix of images \n",
    "        W = weights\n",
    "        b = bias\n",
    "\n",
    "        weight one ** with pixel one \n",
    "        '''\n",
    "        #out = (X * self.W) + self.b\n",
    "        out = X.reshape(X.shape[0], self.W.shape[0]).dot(self.W) + self.b\n",
    "        # out = \n",
    "        ############################################\n",
    "        #             END OF YOUR CODE             #\n",
    "        ############################################\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Restore dimensions before flattening operation\n",
    "            \n",
    "        Args:\n",
    "            dout: Derivation of the local out\n",
    "    \n",
    "        Returns:\n",
    "            dX : Derivation with respect to X\n",
    "            dW : Derivation with respect to W\n",
    "            db : Derivation with respect to b\n",
    "        '''\n",
    "        ############################################\n",
    "        #                   TODO                   #    \n",
    "        ############################################\n",
    "\n",
    "        dX = dout.dot(self.W.T).reshape(self.X.shape)\n",
    "        dW = self.X.reshape(self.X.shape[0], self.W.shape[0]).T.dot(dout)\n",
    "        ############################################\n",
    "        #             END OF YOUR CODE             #\n",
    "        ############################################        \n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dX, [dW, db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0n/n106l5rn7cb6yw8038g17vcm0000gn/T/ipykernel_45480/3524297331.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "Flatten(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'out_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0n/n106l5rn7cb6yw8038g17vcm0000gn/T/ipykernel_45480/2795712828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFullyConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'out_size'"
     ]
    }
   ],
   "source": [
    "FullyConnected(train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Testing\n",
    "\n",
    "Once you've connected many types of layers in a network and you notice an error in your training, it can be difficult to track down which layer exactly has a buggy implementation. Since you're implementing each layer in a modular fashion you can also test them individually. So, it's a good practice to write tests for each of your layers at this point already.\n",
    "\n",
    "\n",
    "There are properties you know should hold true about the input and output of your layer. In the `FullyConnected` layer, you may want to test:\n",
    "* In the forward pass: which shape should the return value have?\n",
    "* In the backward pass: which shape should the derivatives `dX`, `dW` and `db` have?\n",
    "* In the backward pass: which shape do you expect from the argument `dout`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Define Activation Function\n",
    "First, remember that activation functions are non-linearities added to your architecture. As an example the classic ReLU function is implemented here: \n",
    "\n",
    "$$\n",
    "f ( x ) = \\left\\{ \\begin{array} { l l } { x } & { \\text { if } x > 0 } \\\\ { 0 } & { \\text { otherwise } } \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "The ReLU function matches the current weight initialization in the fully-connected layer. Note that may have to be changed if you implement other activation functions. \n",
    "\n",
    "Actually, the activation function belongs into the `layer.py`, for the sake of clarity, however, the functions are put into a separate file `activation_func.py`.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the `ReLU` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(object):\n",
    "    ''' Implements activation function rectified linear unit (ReLU) \n",
    "    \n",
    "    ReLU activation function is defined as the positive part of \n",
    "    its argument. \n",
    "    (paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6419&rep=rep1&type=pdf)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' In the forward pass return the identity for x > 0\n",
    "        \n",
    "        Safe input for backprop and forward all values that are above 0.\n",
    "        '''\n",
    "        self.X = X\n",
    "        ############################################\n",
    "        #                   TODO                   #    \n",
    "        ############################################\n",
    "\n",
    "        raise NotImplementedError(\"Your task...\")\n",
    "        #return        \n",
    "        ############################################\n",
    "        #             END OF YOUR CODE             #\n",
    "        ############################################\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Derivative of ReLU\n",
    "        \n",
    "        Retruns:\n",
    "            dX: for all x \\elem X <= 0 in forward pass \n",
    "                return 0 else x\n",
    "            []: no gradients on ReLU operation\n",
    "        '''\n",
    "        dX = dout.copy()\n",
    "        ############################################\n",
    "        #                   TODO                   #    \n",
    "        ############################################\n",
    "        raise NotImplementedError(\"Your task...\")\n",
    "        #dX =\n",
    "        ############################################\n",
    "        #             END OF YOUR CODE             #\n",
    "        ############################################\n",
    "        return dX, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork Class\n",
    "\n",
    "A `NeuralNetwork object` connects all layers and activation functions of a model architecture using the `forward` and `backward` methods of the containing objects. Calling `forward` on the `NeuralNetwork object` will pass a given input through the whole computational graph. The `backward` function calculates the gradients via backpropagation. \n",
    "\n",
    "It further creates a global list of all parameters in the network during initialization, which is used later in optimization process.  \n",
    "\n",
    "A `predict` function implements a foward pass with the application of a given score function at the end of the calculation. At the momennt it is suited for the softmax function, taking only the *max* argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    ''' Creates a neural network from a given layer architecture \n",
    "    \n",
    "    This class is suited for fully-connected network and\n",
    "    convolutional neural network architectures. It connects \n",
    "    the layers and passes the data from one end to another.\n",
    "    '''\n",
    "    def __init__(self, layers, score_func=None):\n",
    "        ''' Setup a global parameter list and initilize a\n",
    "            score function that is used for predictions.\n",
    "        \n",
    "        Args:\n",
    "            layer: neural network architecture based on layer and activation function objects\n",
    "            score_func: function that is used as classifier on the output\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params.append(layer.params)\n",
    "        self.score_func = score_func\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Pass input X through all layers in the network \n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dout):\n",
    "        grads = []\n",
    "        ''' Backprop through the network and keep a list of the gradients\n",
    "            from each layer.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            dout, grad = layer.backward(dout)\n",
    "            grads.append(grad)\n",
    "        return grads\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Run a forward pass and use the score function to classify \n",
    "            the output.\n",
    "        '''\n",
    "        X = self.forward(X)\n",
    "        return np.argmax(self.score_func(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Define a Cost Function\n",
    "Implementations of different cost functions should be placed into `cost_func.py`. A cost function object defines the criteria your network is evaluating during the optimization process. Further the class contains score functions that can be used as classification criteria for predictions using a certain model. So it is necessary to provide a cost function object to a optimization algorithm for the training process.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the `softmax` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class CostCriteria(object):\n",
    "    ''' Implements different types of loss and score functions for neural networks\n",
    "    \n",
    "    Todo:\n",
    "        - Implement init that defines score and loss function \n",
    "    '''\n",
    "    def softmax(X):\n",
    "        ''' Numeric stable calculation of softmax\n",
    "        '''\n",
    "        ############################################\n",
    "        #                   TODO                   #    \n",
    "        ############################################\n",
    "        raise NotImplementedError(\"Your task...\")\n",
    "        #return =\n",
    "        ############################################\n",
    "        #             END OF YOUR CODE             #\n",
    "        ############################################        \n",
    "    \n",
    "    def cross_entropy_softmax(X, y):\n",
    "        ''' Computes loss and prepares dout for backprop \n",
    "\n",
    "        https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        '''\n",
    "        m = y.shape[0]\n",
    "        p = CostCriteria.softmax(X)\n",
    "        log_likelihood = -np.log(p[range(m), y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        dout = p.copy()\n",
    "        dout[range(m), y] -= 1\n",
    "        return loss, dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Testing\n",
    "Softmax turns each row (each sample) into a probability distribution over the output classes. So you may want to test\n",
    "* the shape of the return value should contain the same number of samples\n",
    "* if each row is a valid probability distribution. So all values should of the return value should be [0..1] and each row should sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with SGD\n",
    "The file `optimizer.py` contains implementations of optimization algorithms. Your optimizer needs your custom `network`, `data` and `loss function` and some additional hyperparameter as arguments to optimize your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):   \n",
    "    def get_minibatches(X, y, batch_size):\n",
    "        ''' Decomposes data set into small subsets (batches)\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        batches = []\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X[i:i + batch_size, :, :, :]\n",
    "            y_batch = y[i:i + batch_size, ]\n",
    "            batches.append((X_batch, y_batch))\n",
    "        return batches    \n",
    "\n",
    "    def sgd(network, X_train, y_train, cost_function, batch_size=32, epoch=100, learning_rate=0.001, X_test=None, y_test=None, verbose=None):\n",
    "        ''' Optimize a given network with stochastical gradient descent\n",
    "        \n",
    "        Args:\n",
    "            X_train: trainings data\n",
    "            y_train: trainings label (ground truth)\n",
    "            cost_function: cost function\n",
    "            batch_size: size of a single batch\n",
    "            epoch: amount of epochs\n",
    "            learning_rate: the rate which is going to be multiplied with the gradient\n",
    "            X_test: trainings data if you want to test your model in each epcoh\n",
    "            y_test: trainings labels\n",
    "            verbose: if set it prints out training accuracy and test accuracy\n",
    "        Returns:\n",
    "            Model with optimized parameters\n",
    "        '''\n",
    "        minibatches = Optimizer.get_minibatches(X_train, y_train, batch_size)\n",
    "        for i in range(epoch):\n",
    "            loss = 0\n",
    "            if verbose:\n",
    "                print('Epoch',i)\n",
    "            for X_mini, y_mini in minibatches:\n",
    "                # calculate loss and derivation of the last layer\n",
    "                loss, dout = cost_function(network.forward(X_mini), y_mini)\n",
    "                # Do not train in epoch 0, so we now performance b4 training\n",
    "                if i > 0:\n",
    "                    # calculate gradients via backpropagation\n",
    "                    grads = network.backward(dout)\n",
    "                    # run vanilla sgd update for all learnable parameters in self.params\n",
    "                    for param, grad in zip(network.params, reversed(grads)):\n",
    "                        for i in range(len(grad)):\n",
    "                            param[i] += - learning_rate * grad[i]\n",
    "            if verbose:\n",
    "                train_acc = np.mean(y_train == network.predict(X_train))\n",
    "                test_acc = np.mean(y_test == network.predict(X_test))                                \n",
    "                print(\"Loss = {0} :: Training = {1} :: Test = {2}\".format(loss, train_acc, test_acc))\n",
    "        return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it All Together\n",
    "Now you have to put all parts together to create and train a fully connected neural network. First, you have to define an individual network architecture by flattening the input and stacking fully-connected layer with activation functions, e.g.:\n",
    "\n",
    "**Input -> Flatten -> Dense -> Activation -> Dense -> Activation -> Dense -> Activation -> Dense**\n",
    "\n",
    "You have to initialize all objects you need to build your custom architecture and put them into a `list` afterward. Your architecture is then given to a `NeuralNetwork` object that handles the inter-layer communication during the forward and backward pass. It will also set the evaluation criteria at the end of the network, because of that you end your architecture with a fully-connected layer. The pipeline above is implemented in the following cell.\n",
    "\n",
    "Finally, you can train the model with an optimization algorithm and a cost function, here stochastic gradient descent and cross-entropy with softmax. That kind of pipeline is similar to the one you would create with a more sophisticated framework like Tensorflow or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_mnist():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(784, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn = NeuralNetwork(fcn_mnist(), score_func=CostCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn = Optimizer.sgd(fcn, train_images, train_labels, CostCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.01, X_test=test_images, y_test=test_labels, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Experiment with the Framework\n",
    "Here is your last exercise for this notebook: \n",
    "\n",
    "Now you have a basic idea of how to build a fully-connected neural network with the framework. The next steps are straight forward. Download all script files of the framework from Moodle or the [exercise-repository](https://git.tools.f4.htw-berlin.de/ben/1163150_exercises) and move the implemented functions into the correct script files: \n",
    "\n",
    "- `layer.py`\n",
    "- `activation_func.py`\n",
    "- `cost_func`\n",
    "\n",
    "After chose a dataset you like and create a data loader in the script file `utils.py`. Load your data, build a neural network and try to build a good classifier. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "Neural Networks - Exercise: Simple MNIST Network <br/>\n",
    "by Benjamin Voigt <br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at [https://gitlab.com/deep.TEACHING](https://gitlab.com/deep.TEACHING).\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Benjamin Voigt\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "087ed4b5ae5ab8a802fc70b7204961b469aa2ba57c16f25f9d870b586c21be65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
